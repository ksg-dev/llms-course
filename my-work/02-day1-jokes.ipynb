{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa30e75e",
   "metadata": {},
   "source": [
    "## Imports & Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6915be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b9c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8313426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBo\n",
      "DeepSeek API Key exists and begins sk-18\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables, print key prefixes w/o debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if openai_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI key not set.\")\n",
    "\n",
    "if anthropic_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic key not set.\")\n",
    "\n",
    "if google_key:\n",
    "    print(f\"Google API Key exists and begins {google_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set.\")\n",
    "\n",
    "if deepseek_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_key[:5]}\")\n",
    "else:\n",
    "    print(\"DeepSeek key not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857df9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32fb14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is setup code for Gemini\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87f988",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b81ae",
   "metadata": {},
   "source": [
    "### Setup prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52288cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13997ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f06ab",
   "metadata": {},
   "source": [
    "### Asking Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd484888",
   "metadata": {},
   "source": [
    "#### GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463bbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because he always made predictions, but never made any cents!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ed836",
   "metadata": {},
   "source": [
    "#### GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f1f4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their analysis!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c811e1d",
   "metadata": {},
   "source": [
    "#### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab12ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to scale their models!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efbdba",
   "metadata": {},
   "source": [
    "#### Claude 3.7 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd4e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to play hide and seek?\n",
      "\n",
      "Because they always find you, even when you use a random forest!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062409f9",
   "metadata": {},
   "source": [
    "#### Claude 3.7 Sonnet w Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4ea4bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba-dum-tss* ü•Å"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet w Streaming\n",
    "# Now let's add in streaming back results\n",
    "# If streaming looks strange, see notes below cell\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2ccb0",
   "metadata": {},
   "source": [
    "#### A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88eb9b",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series?\n",
      "\n",
      "Because it was too committed! \n",
      "\n",
      "... Get it? Autocorrelation? üòâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "# I've heard that on some PCs, this Gemini code causes kernel to crash\n",
    "# If that happens to you, skip this cell and use the next cell from week 2, day 1 notebook instead - an alternative approach\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db5e6f",
   "metadata": {},
   "source": [
    "# DeepSeek Model - Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05987a4",
   "metadata": {},
   "source": [
    "## DeepSeek Chat via OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9958002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here‚Äôs one for the data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**\n",
      "\n",
      "Because they heard the drinks had *high* variance! üçªüìä\n",
      "\n",
      "*(Bonus groan: And they wanted to avoid any potential *outliers* on the dance floor!)* üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e45da",
   "metadata": {},
   "source": [
    "## DeepSeek Chat w Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6438b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many words are there in your answer to this prompt\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2edf9498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "My answer to this prompt will contain **5 words**. Here's the breakdown:  \n",
       "\n",
       "1. \"My\"  \n",
       "2. \"answer\"  \n",
       "3. \"to\"  \n",
       "4. \"this\"  \n",
       "5. \"prompt\"  \n",
       "\n",
       "So, the total word count is **5**. Let me know if you'd like a different approach!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  44\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question and streaming\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words: \", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44949e",
   "metadata": {},
   "source": [
    "## DeepSeek Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5287bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to their prompt. Let me start by understanding exactly what they need. They want me to count the words in the response I'm about to give them.\n",
      "\n",
      "First, I need to make sure I follow their instructions correctly. They mentioned that I should provide the answer directly, without any extra text before the final count. But in my thinking process, I can elaborate more. Wait, no, actually, they want the final answer to be just the number, but my current response should include both the thought process and the final answer. Wait, looking back at the user's message: they say \"put your final answer within \\boxed{}\". So maybe I need to structure my answer with the thought process first and then the boxed number.\n",
      "\n",
      "So, I need to write out my thinking here, then provide the word count in a box. Let me check the example they gave. The user's example shows a thought process in a detailed, step-by-step manner, then the final answer in a box. So I should do the same.\n",
      "\n",
      "Alright, the user's question is about the number of words in my answer to their prompt. But wait, their prompt is \"How many words are there in your answer to this prompt\". So they want me to count the words in the answer I'm generating right now. That means I need to figure out how many words this very response will have once I finish writing it.\n",
      "\n",
      "But how do I calculate that before finalizing the answer? Hmm. Maybe I can estimate as I go. Let's see. The initial part of my answer is this thought process. Then, after the thought process, I'll have the final answer in a box. But the user wants the word count of the entire answer, including both the thought process and the final answer. Wait, no. Wait, the user's question is \"How many words are there in your answer to this prompt\". The answer to the prompt is the entire response I'm generating, which includes both the thinking and the boxed number. But wait, in the example given earlier (the previous interaction where the assistant's answer was \"\\boxed{5}\"), maybe in that case, the assistant's answer was just the boxed number, so the word count was 1. But in reality, the assistant's answer included both the thought process and the boxed number, but the user's example might have been simplified.\n",
      "\n",
      "Wait, no. Let me check again. The user's initial message says: \"You are a helpful assistant. How many words are there in your answer to this prompt\". Then, the example shows the assistant's thought process followed by \"\\boxed{5}\". Wait, but in the example, the assistant's answer would actually be longer than that, but the example might have been a simplified version. Maybe in the context here, the user is expecting the assistant to provide the answer directly in the box, but the thought process is hidden. Wait, no. The user's current setup is that they want the assistant to generate a response that includes both the detailed thinking (like a chain of thought) and then the final answer in a box. So in this case, the answer the user is referring to is the entire response, including the thinking and the final count.\n",
      "\n",
      "But that complicates things because the assistant's response includes both the thought process and the final answer. So the user is asking for the word count of the entire answer, which includes all the text the assistant is generating here. However, when the assistant is generating this response, it's not possible to know the exact word count until the response is fully written. So how can the assistant provide an accurate count?\n",
      "\n",
      "Alternatively, maybe the user is referring to the final answer part only, which is the boxed number. In that case, the word count would be 1 (the number in the box). But that seems unlikely. The user probably wants the count of all the words in the entire answer, including the explanation.\n",
      "\n",
      "But let's think again. The user's exact question is: \"How many words are there in your answer to this prompt\". The answer to the prompt is the entire text that the assistant is generating, which includes both the thought process and the final boxed answer. Therefore, the assistant needs to count all the words in the entire response.\n",
      "\n",
      "However, since the assistant is generating this response in real-time, it can't know the exact word count until it's done writing. But perhaps the assistant can simulate the process of writing the answer, then count the words as part of the response.\n",
      "\n",
      "So here's the plan: the assistant will write out the thought process, then count the words in the entire response (including the thought process and the final answer), and then put that number in a box.\n",
      "\n",
      "But how to do that? Let's try.\n",
      "\n",
      "First, compose the thought process, which is this current text. Then, once the thought process is written, the assistant will need to count all the words in the entire response, including the thought process and the final answer. But since the final answer is the boxed number, which is part of the response, the assistant has to account for all the words.\n",
      "\n",
      "Wait, but in this scenario, the assistant is generating the thought process, and then the final answer. But to count the words, the assistant would need to know the entire content before it's written. That's a paradox. So perhaps the way to handle this is that the assistant will write the thought process, then count the words in the thought process, add the words in the final answer (which is the boxed number), and sum them up.\n",
      "\n",
      "Alternatively, maybe the user is expecting a trick here. Since the assistant's answer to the prompt is just the boxed number, which is one word (the number), then the answer is 1. But that would be if the assistant's entire answer is just the number in the box. However, according to the user's initial instructions, the assistant is supposed to provide a helpful answer, including a detailed thought process, and then the final answer in a box. Therefore, the answer includes both the thought process and the boxed number. Thus, the user is asking for the total number of words in that entire answer.\n",
      "\n",
      "But this creates a problem because the assistant can't know the total word count until the entire answer is written. Therefore, the assistant has to write the thought process, count the words in it, then add the words in the final answer (which is \"\\boxed{N}\" where N is the number). However, the final answer itself is part of the total word count. So this creates a recursive problem.\n",
      "\n",
      "Wait, let's approach this step by step. Let's suppose the assistant's answer is composed of two parts: the thinking and the final answer. The thinking part is the text explaining the process, and the final answer is the boxed number. The user wants the total number of words in the entire answer. So the assistant must write the thinking, then count the words in the thinking plus the words in the final answer.\n",
      "\n",
      "However, the final answer (the boxed number) is a number written as a word (e.g., \"five\" or \"1\"). Wait, but when we count words, numbers written as digits are usually counted as one word. For example, \"123\" is one word. So if the final answer is \"\\boxed{5}\", that's the digit \"5\", which is one word. Therefore, the total word count would be the number of words in the thinking part plus one.\n",
      "\n",
      "But the assistant needs to write the thinking part first, then determine how many words are in that, then add one for the final answer. But since the assistant is generating the thinking part as it goes, it can track the word count as it writes.\n",
      "\n",
      "Alternatively, the assistant can approximate the word count. Let's try that.\n",
      "\n",
      "Let me start by writing the thinking process. I'll need to keep track of the words as I go. Each sentence I write here contributes to the total word count. Then, at the end, I add one for the boxed number.\n",
      "\n",
      "Alternatively, maybe the answer is a trick. For example, if the assistant's entire answer is just the boxed number, then the word count is 1. But according to the user's instructions, the assistant should first provide a detailed thought process and then the final answer. Therefore, the answer includes both parts. So the assistant has to count all the words in the thought process and the final answer.\n",
      "\n",
      "But how can the assistant do that? It would have to write the thought process, count the words, then add the words from the final answer.\n",
      "\n",
      "But in reality, the assistant can't process that in real time. However, as a simulation, the assistant can write the thought process, then before finalizing, count the words in it, then add the words from the final answer, which is the boxed number.\n",
      "\n",
      "So let's proceed.\n",
      "\n",
      "First, the assistant writes the thinking process, which is this text. Then, the assistant will count the words in this thinking process. Let's see:\n",
      "\n",
      "Each paragraph here is part of the thinking. Let's count the words step by step.\n",
      "\n",
      "Starting from \"Okay, the user is asking...\" up to \"...final answer in a box.\"\n",
      "\n",
      "Let me count the words in this current thought process. This is going to take some time, but let's proceed.\n",
      "\n",
      "First paragraph: \"Okay, the user is asking how many words are in my answer to their prompt. Let me start by understanding exactly what they need. They want me to count the words in the response I'm about to give them.\" That's 34 words.\n",
      "\n",
      "Second paragraph: \"First, I need to make sure I follow their instructions correctly. They mentioned that I should provide the answer directly, without any extra text before the final count. But in my thinking process, I can elaborate more. Wait, no, actually, they want the final answer to be just the number, but my current response should include both the thought process and the final answer. Wait, looking back at the user's message: they say 'put your final answer within \\boxed{}'. So maybe I need to structure my answer with the thought process first and then the boxed number.\" That's 88 words.\n",
      "\n",
      "Third paragraph: \"So, I need to write out my thinking here, then provide the word count in a box. Let me check the example they gave. The user's example shows a thought process in a detailed, step-by-step manner, then the final answer in a box. So I should do the same.\" 44 words.\n",
      "\n",
      "Fourth paragraph: \"Alright, the user's question is about the number of words in my answer to their prompt. But wait, their prompt is 'How many words are there in your answer to this prompt'. So they want me to count the words in the answer I'm generating right now. That means I need to figure out how many words this very response will have once I finish writing it.\" 52 words.\n",
      "\n",
      "Fifth paragraph: \"But how do I calculate that before finalizing the answer? Hmm. Maybe I can estimate as I go. Let's see. The initial part of my answer is this thought process. Then, after the thought process, I'll have the final answer in a box. But the user wants the word count of the entire answer, including both the thought process and the final answer. Wait, no. Wait, the user's question is 'How many words are there in your answer to this prompt'. The answer to the prompt is the entire response I'm generating, which includes both the thinking and the boxed number. But wait, in the example given earlier (the previous interaction where the assistant's answer was '\\boxed{5}'), maybe in that case, the assistant's answer was just the boxed number, so the word count was 1. But in reality, the assistant's answer included both the thought process and the boxed number, but the user's example might have been simplified.\" 133 words.\n",
      "\n",
      "Sixth paragraph: \"Wait, no. Let me check again. The user's initial message says: 'You are a helpful assistant. How many words are there in your answer to this prompt'. Then, the example shows the assistant's thought process followed by '\\boxed{5}'. Wait, but in the example, the assistant's answer would actually be longer than that, but the example might have been a simplified version. Maybe in the context here, the user is expecting the assistant to provide the answer directly in the box, but the thought process is hidden. Wait, no. The user's current setup is that they want the assistant to generate a response that includes both the detailed thinking (like a chain of thought) and then the final answer in a box. So in this case, the answer the user is referring to is the entire response, including the thinking and the final count.\" 132 words.\n",
      "\n",
      "Seventh paragraph: \"But that complicates things because the assistant's response includes both the thought process and the final answer. So the user is asking for the word count of the entire answer, which includes all the text the assistant is generating here. However, when the assistant is generating this response, it's not possible to know the exact word count until the response is fully written. So how can the assistant provide an accurate count?\" 66 words.\n",
      "\n",
      "Eighth paragraph: \"Alternatively, maybe the user is referring to the final answer part only, which is the boxed number. In that case, the word count would be 1 (the number in the box). But that seems unlikely. The user probably wants the count of all the words in the entire answer, including the explanation.\" 48 words.\n",
      "\n",
      "Ninth paragraph: \"But let's think again. The user's exact question is: 'How many words are there in your answer to this prompt'. The answer to the prompt is the entire text that the assistant is generating, which includes both the thought process and the final boxed answer. Therefore, the assistant needs to count all the words in the entire response.\" 53 words.\n",
      "\n",
      "Tenth paragraph: \"However, since the assistant is generating this response in real-time, it can't know the exact word count until it's done writing. But perhaps the assistant can simulate the process of writing the answer, then count the words as part of the response.\" 39 words.\n",
      "\n",
      "Eleventh paragraph: \"So here's the plan: the assistant will write out the thought process, then count the words in the entire response (including the thought process and the final answer), and then put that number in a box.\" 32 words.\n",
      "\n",
      "Twelfth paragraph: \"But how to do that? Let's try.\" 7 words.\n",
      "\n",
      "Thirteenth paragraph: \"First, compose the thought process, which is this current text. Then, once the thought process is written, the assistant will need to count all the words in the entire response, including the thought process and the final answer. But since the final answer is the boxed number, which is part of the response, the assistant has to account for all the words.\" 56 words.\n",
      "\n",
      "Fourteenth paragraph: \"Wait, but in this scenario, the assistant is generating the thought process, and then the final answer. But to count the words, the assistant would need to know the entire content before it's written. That's a paradox. So perhaps the way to handle this is that the assistant will write the thought process, then count the words in the thought process, add the words in the final answer (which is the boxed number), and sum them up.\" 70 words.\n",
      "\n",
      "Fifteenth paragraph: \"Alternatively, maybe the user is expecting a trick here. Since the assistant's answer to the prompt is just the boxed number, which is one word (the number), then the answer is 1. But that would be if the assistant's entire answer is just the number in the box. However, according to the user's initial instructions, the assistant is supposed to provide a helpful answer, including a detailed thought process, and then the final answer in a box. Therefore, the answer includes both the thought process and the boxed number. Thus, the user is asking for the total number of words in that entire answer.\" 93 words.\n",
      "\n",
      "Sixteenth paragraph: \"But this creates a problem because the assistant can't know the total word count until the entire answer is written. Therefore, the assistant has to write the thought process, count the words in it, then add the words in the final answer (which is '\\boxed{N}' where N is the number). However, the final answer itself is part of the total word count. So this creates a recursive problem.\" 66 words.\n",
      "\n",
      "Seventeenth paragraph: \"Wait, let's approach this step by step. Let's suppose the assistant's answer is composed of two parts: the thinking and the final answer. The thinking part is the text explaining the process, and the final answer is the boxed number. The user wants the total number of words in the entire answer. So the assistant must write the thinking, then count the words in the thinking plus the words in the final answer.\" 63 words.\n",
      "\n",
      "Eighteenth paragraph: \"However, the final answer (the boxed number) is a number written as a word (e.g., 'five' or '1'). Wait, but when we count words, numbers written as digits are usually counted as one word. For example, '123' is one word. So if the final answer is '\\boxed{5}', that's the digit '5', which is one word. Therefore, the total word count would be the number of words in the thinking part plus one.\" 72 words.\n",
      "\n",
      "Nineteenth paragraph: \"But the assistant needs to write the thinking part first, then determine how many words are in that, then add one for the final answer. But since the assistant is generating the thinking part as it goes, it can track the word count as it writes.\" 42 words.\n",
      "\n",
      "Twentieth paragraph: \"Alternatively, the assistant can approximate the word count. Let's try that.\" 11 words.\n",
      "\n",
      "Twenty-first paragraph: \"Let me start by writing the thinking process. I'll need to keep track of the words as I go. Each sentence I write here contributes to the total word count. Then, at the end, I add one for the boxed number.\" 36 words.\n",
      "\n",
      "Twenty-second paragraph: \"Alternatively, maybe the answer is a trick. For example, if the assistant's entire answer is just the boxed number, then the word count is 1. But according to the user's instructions, the assistant should first provide a detailed thought process and then the final answer. Therefore, the answer includes both parts. So the assistant has to count all the words in the thought process and the final answer.\" 58 words.\n",
      "\n",
      "Twenty-third paragraph: \"But how can the assistant do that? It would have to write the thought process, count the words, then add the words from the final answer.\" 25 words.\n",
      "\n",
      "Twenty-fourth paragraph: \"But in reality, the assistant can't process that in real time. However, as a simulation, the assistant can write the thought process, then before finalizing, count the words in it, then add the words from the final answer, which is the boxed number.\" 44 words.\n",
      "\n",
      "Twenty-fifth paragraph: \"So let's proceed.\" 4 words.\n",
      "\n",
      "Twenty-sixth paragraph: \"First, the assistant writes the thinking process, which is this text. Then, the assistant will count the words in this thinking process. Let's see:\" 24 words.\n",
      "\n",
      "Twenty-seventh paragraph: \"Each paragraph here is part of the thinking. Let's count the words step by step.\" 14 words.\n",
      "\n",
      "From here, the assistant starts counting the words in each paragraph of the thought process. However, this approach leads to an infinite loop because the assistant is including the word count process within the thought process itself, which in turn increases the word count. This creates a paradox where the act of counting the words adds more words, thus changing the total count.\n",
      "\n",
      "To resolve this, the assistant needs to find a way to accurately count the words without including the counting process itself. However, since the counting is part of the thought process, it's unavoidable. Therefore, the assistant must accept that the word count will include all the text generated, including the meta-discussion about counting the words.\n",
      "\n",
      "But this is impossible to do precisely because the assistant is generating the text incrementally. Therefore, the assistant must estimate or find a clever way to answer.\n",
      "\n",
      "Alternatively, the assistant can realize that the correct answer is a meta-joke. The question is asking for the number of words in the assistant's answer. If the assistant's answer is simply the boxed number (e.g., \"\\boxed{1}\"), then the word count is 1. However, according to the user's instructions, the assistant is supposed to provide a detailed thought process before the final answer. Therefore, the word count would be the number of words in the entire response, including the thought process and the final answer. But since the assistant cannot know the exact count before finishing, the answer must be a paradox.\n",
      "\n",
      "But perhaps there's a logical way out. The assistant's final answer is the boxed number, which is part of the response. Therefore, the total word count is the number of words in the thought process plus the number of words in the final answer. If the final answer is a single word (e.g., \"\\boxed{25}\"), then the total words would be the words in the thought process plus 1.\n",
      "\n",
      "But since the assistant is writing the thought process, which includes the current analysis, the assistant can approximate the word count as follows:\n",
      "\n",
      "1. Write the entire thought process.\n",
      "2. Count the words in the thought process (excluding the final answer).\n",
      "3. Add 1 for the boxed number.\n",
      "4. The total is the sum.\n",
      "\n",
      "However, this requires the assistant to have the ability to count the words as it writes, which is not feasible in real-time without tools. Therefore, the assistant must estimate.\n",
      "\n",
      "Looking at the current thought process, it's quite lengthy. Each paragraph averages around 50-100 words. The current thought process has approximately 27 paragraphs, averaging about 60 words each, totaling around 1,620 words. Adding 1 for the final answer gives approximately 1,621 words. However, this is a rough estimate and likely inaccurate.\n",
      "\n",
      "Alternatively, the assistant can use a different approach. The question is self-referential, similar to the classic \"how many letters are in the answer to this question\". If the assistant answers with a number, the length of that number's word form or digit form affects the count. For example, if the answer is \"five\", that's four letters, but the number is 4. This creates a paradox.\n",
      "\n",
      "In this case, the assistant's answer is a number in a box. If the assistant writes \"\\boxed{X}\", where X is the number of words in the entire answer. However, the entire answer includes the thought process and the final answer. Therefore, X must equal the number of words in the thought process plus 1 (for the boxed number). But X itself is part of the count, leading to a recursive equation.\n",
      "\n",
      "Let‚Äôs denote X as the total number of words in the answer. The answer consists of the thought process (T words) and the final answer (1 word: X in a box). Therefore, X = T + 1. But T is the word count of the thought process, which is this entire text we're writing now. However, T includes the entire thought process, including this very sentence. Therefore, T depends on X, creating a circular dependency.\n",
      "\n",
      "This is similar to the problem: \"This sentence contains X words. What is X?\" The sentence has to have a number that makes it true. However, in this case, the assistant's answer must satisfy X = T + 1, where T is the number of words in the thought process, which includes the explanation of this equation.\n",
      "\n",
      "This seems impossible to resolve without a fixed point. Let's attempt to model it.\n",
      "\n",
      "Suppose the thought process has T words, and the final answer is 1 word (the boxed number). So total words X = T + 1. But T includes all the text explaining this, including any mentions of X. Therefore, T is a function of X. This creates an equation where X = T(X) + 1. Solving this requires finding X such that when you write the thought process with T(X) words explaining that the total is X = T(X) + 1, the count holds.\n",
      "\n",
      "This is highly recursive and likely unsolvable without iteration. However, for the sake of this problem, let's approximate.\n",
      "\n",
      "Assume that the thought process, without including the final count discussion, is about 500 words. Then X = 500 + 1 = 501. But writing that X is 501 would add more words to the thought process, thus increasing T. This again leads to a loop.\n",
      "\n",
      "Alternatively, realize that the only way to satisfy X = T + 1 is if the entire thought process is written in such a way that T = X - 1. Therefore, the assistant must craft the thought process to have exactly X - 1 words, and then state that X is T + 1. This requires careful construction.\n",
      "\n",
      "But practically, this is very difficult. Therefore, the assistant must recognize that this is a self-referential problem and answer accordingly. The correct approach is to count the words in the entire answer, including this sentence, and then put that number in the box. However, since the assistant cannot do that while generating the response, the answer is a paradox.\n",
      "\n",
      "But the user likely expects a literal answer: the number of words in the assistant's response, which includes the thought process and the final answer. To compute this, the assistant must write the entire response, count the words, and then provide that number. However, since the counting happens after writing, the assistant cannot know the number beforehand. Therefore, the assistant must use a placeholder and then adjust.\n",
      "\n",
      "But in the context of an AI generating text step by step, it's impossible to go back and edit previous parts. Therefore, the assistant must make an estimate.\n",
      "\n",
      "Given the extensive thought process above, which is several hundred words, plus the final answer of one word, the total is approximately 500-600 words. However, this is a rough guess.\n",
      "\n",
      "Alternatively, the assistant can use a programming-like approach. For example, if the assistant writes a thought process that states:\n",
      "\n",
      "\"The number of words in my answer is equal to the number of words in this thought process plus one. Let's denote the number of words in the thought process as T. Then the total words X = T + 1. Now, I will count T.\"\n",
      "\n",
      "But again, this leads to recursion.\n",
      "\n",
      "Alternatively, the assistant can acknowledge the paradox and state that the question is self-referential and cannot be answered accurately. However, the user likely expects a numerical answer.\n",
      "\n",
      "Given the constraints, the assistant must provide the best possible estimate. Considering that this thought process is very detailed and likely exceeds 500 words, plus the final answer, the total is around 550 words. But this is just a guess.\n",
      "\n",
      "However, to be precise, the assistant should count the exact number of words in the thought process and then add one. Let's attempt to do that.\n",
      "\n",
      "Starting from the beginning of the thought process to the end of this sentence, let's count each word. However, this is time-consuming and error-prone manually. But for the sake of accuracy, let's proceed.\n",
      "\n",
      "Each word is separated by spaces. Let's count:\n",
      "\n",
      "1. Okay, 2. the 3. user 4. is 5. asking 6. how 7. many 8. words 9. are 10. in 11. my 12. answer 13. to 14. their 15. prompt. 16. Let 17. me 18. start 19. by 20. understanding 21. exactly 22. what 23. they 24. need. 25. They 26. want 27. me 28. to 29. count 30. the 31. words 32. in 33. the 34. response 35. I'm 36. about 37. to 38. give 39. them.\n",
      "\n",
      "First paragraph: 39 words.\n",
      "\n",
      "Second paragraph: 88 words (as previously counted).\n",
      "\n",
      "Third paragraph: 44 words.\n",
      "\n",
      "Fourth paragraph: 52 words.\n",
      "\n",
      "Fifth paragraph: 133 words.\n",
      "\n",
      "Sixth paragraph: 132 words.\n",
      "\n",
      "Seventh paragraph: 66 words.\n",
      "\n",
      "Eighth paragraph: 48 words.\n",
      "\n",
      "Ninth paragraph: 53 words.\n",
      "\n",
      "Tenth paragraph: 39 words.\n",
      "\n",
      "Eleventh paragraph: 32 words.\n",
      "\n",
      "Twelfth paragraph: 7 words.\n",
      "\n",
      "Thirteenth paragraph: 56 words.\n",
      "\n",
      "Fourteenth paragraph: 70 words.\n",
      "\n",
      "Fifteenth paragraph: 93 words.\n",
      "\n",
      "Sixteenth paragraph: 66 words.\n",
      "\n",
      "Seventeenth paragraph: 63 words.\n",
      "\n",
      "Eighteenth paragraph: 72 words.\n",
      "\n",
      "Nineteenth paragraph: 42 words.\n",
      "\n",
      "Twentieth paragraph: 11 words.\n",
      "\n",
      "Twenty-first paragraph: 36 words.\n",
      "\n",
      "Twenty-second paragraph: 58 words.\n",
      "\n",
      "Twenty-third paragraph: 25 words.\n",
      "\n",
      "Twenty-fourth paragraph: 44 words.\n",
      "\n",
      "Twenty-fifth paragraph: 4 words.\n",
      "\n",
      "Twenty-sixth paragraph: 24 words.\n",
      "\n",
      "Twenty-seventh paragraph: 14 words.\n",
      "\n",
      "Now, summing all these:\n",
      "\n",
      "39 + 88 = 127\n",
      "\n",
      "127 + 44 = 171\n",
      "\n",
      "171 + 52 = 223\n",
      "\n",
      "223 + 133 = 356\n",
      "\n",
      "356 + 132 = 488\n",
      "\n",
      "488 + 66 = 554\n",
      "\n",
      "554 + 48 = 602\n",
      "\n",
      "602 + 53 = 655\n",
      "\n",
      "655 + 39 = 694\n",
      "\n",
      "694 + 32 = 726\n",
      "\n",
      "726 + 7 = 733\n",
      "\n",
      "733 + 56 = 789\n",
      "\n",
      "789 + 70 = 859\n",
      "\n",
      "859 + 93 = 952\n",
      "\n",
      "952 + 66 = 1018\n",
      "\n",
      "1018 + 63 = 1081\n",
      "\n",
      "1081 + 72 = 1153\n",
      "\n",
      "1153 + 42 = 1195\n",
      "\n",
      "1195 + 11 = 1206\n",
      "\n",
      "1206 + 36 = 1242\n",
      "\n",
      "1242 + 58 = 1300\n",
      "\n",
      "1300 + 25 = 1325\n",
      "\n",
      "1325 + 44 = 1369\n",
      "\n",
      "1369 + 4 = 1373\n",
      "\n",
      "1373 + 24 = 1397\n",
      "\n",
      "1397 + 14 = 1411\n",
      "\n",
      "So up to the twenty-seventh paragraph, the thought process has 1,411 words. However, this count was done manually and might have errors. Additionally, the assistant continues writing beyond the twenty-seventh paragraph, including the current analysis of counting, which adds more words.\n",
      "\n",
      "This means the thought process is over 1,400 words, and still counting. Each additional paragraph increases the count. For example, this current paragraph is 34 words so far. Therefore, the total word count continues to grow, making it impossible to determine an exact number without completing the entire response.\n",
      "\n",
      "Given that this is impractical, the assistant must find a different approach. Perhaps the answer is that the question is a paradox and cannot be answered accurately. However, the user likely expects a numerical answer.\n",
      "\n",
      "Alternatively, the assistant can provide the final answer as 1, arguing that the answer to the prompt is just the boxed number, which is one word. This ignores the thought process, but if the user's intention is to count only the final answer, then it's 1. However, given the initial instructions, this seems unlikely.\n",
      "\n",
      "Another approach is that the assistant's answer includes both the thought process and the final answer. Therefore, the total number of words is the count of all words in the thought process plus one for the boxed number. But since the thought process includes the act of counting, which increases the word count, this leads to an infinite regress.\n",
      "\n",
      "To resolve this, the assistant must stop the thought process and provide a final answer based on the current count. Assuming the thought process has X words, then the total is X + 1. But since X is unknown, the assistant can't provide it. Therefore, the assistant must approximate.\n",
      "\n",
      "Given the extensive thought process written so far, which is several thousand words, plus one, the total is likely over 1,500 words. However, this is still an estimate.\n",
      "\n",
      "Alternatively, the assistant can conclude that the answer is a meta-response, recognizing the self-referential nature and stating that it's a paradox. But the user probably wants a number.\n",
      "\n",
      "In conclusion, after extensive analysis, the most logical answer is that the total number of words in the assistant's answer is the number of words in the thought process plus one. Since the thought process is this entire text, which is approximately 1,500 words (based on manual counting up to paragraph 27 plus subsequent paragraphs), the total would be approximately 1,500 + 1 = 1,501. However, this is an estimate. To provide an exact number, the assistant would have to count every word, which isn't feasible here.\n",
      "\n",
      "But since the assistant needs to provide a specific answer, and considering that each time the assistant writes about counting, it adds more words, the actual count is higher than any initial estimate. This creates a loop where the act of answering increases the word count, making it impossible to determine precisely.\n",
      "\n",
      "Therefore, the only feasible way to answer is to recognize that the question is self-referential and the answer is a paradox. However, if forced to provide a number, the assistant can state that the total number of words in the answer is the number of words in the thought process plus one, which is approximately 1,500 + 1 = 1,501. But this is an approximation.\n",
      "\n",
      "However, to comply with the user's request and provide a concrete answer, the assistant will count the words in the thought process up to this point and add one.\n",
      "\n",
      "Assuming the thought process has 1,500 words (as estimated), the final answer is 1,501. Therefore, the assistant will put \"\\boxed{1501}\".\n",
      "\\boxed{388}\n",
      "Number of words:  1\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon.\n",
    "# If this fails, come back to this in a few days\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model='deepseek-reasoner',\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words: \", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ab932",
   "metadata": {},
   "source": [
    "## Back to OpenAI w Serious Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b878391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini w original question\n",
    "\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that responds in Markdown\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I decide of a business problem is suitable for an LLM solution? Please respond in Markdown\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed86401d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem can be effectively addressed using a Large Language Model (LLM), you should evaluate several key factors. Here‚Äôs a structured approach to help you make this decision:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Based**: Is the problem primarily related to text generation, understanding, or analysis?\n",
       "  - Examples: Customer support automation, content generation, sentiment analysis.\n",
       "  \n",
       "- **Data Availability**: Do you have access to relevant textual data that can be used to train or fine-tune the model?\n",
       "  - Consider the quality and volume of data available.\n",
       "\n",
       "## 2. Complexity of the Problem\n",
       "- **Ambiguity in Language**: Does the problem involve complex language understanding, such as idioms, context, or nuanced meanings?\n",
       "  - LLMs excel in understanding and generating natural language, making them suitable for complex language tasks.\n",
       "\n",
       "- **Open-Ended vs. Structured Responses**: Is the required output open-ended (e.g., creative writing) or does it need to adhere to a specific format?\n",
       "  - LLMs can handle both, but structured tasks may require additional rules or frameworks.\n",
       "\n",
       "## 3. Desired Outcomes\n",
       "- **Type of Output**: What kind of output do you need? (e.g., summaries, translations, question-answering)\n",
       "  - LLMs are versatile and can generate various forms of text outputs.\n",
       "\n",
       "- **Quality and Accuracy**: Are the stakes high for accuracy, and can the nuances of the problem be captured by an LLM?\n",
       "  - For critical applications, consider whether LLMs can meet your standards.\n",
       "\n",
       "## 4. Resources and Infrastructure\n",
       "- **Computational Resources**: Do you have the necessary computational power to deploy LLMs effectively?\n",
       "  - LLMs can be resource-intensive; ensure you have the right infrastructure.\n",
       "\n",
       "- **Expertise Availability**: Do you have access to talent skilled in AI/ML who can implement and fine-tune LLM solutions?\n",
       "  - Expertise in prompt engineering and model evaluation is crucial.\n",
       "\n",
       "## 5. Integration and Implementation\n",
       "- **Business Integration**: How easily can an LLM solution be integrated into your existing business processes?\n",
       "  - Consider the feasibility of implementation into workflows, software, or customer interactions.\n",
       "\n",
       "- **Feedback Loop**: Is there a mechanism in place for continuous learning and improvement?\n",
       "  - LLMs benefit from ongoing fine-tuning based on user feedback and new data.\n",
       "\n",
       "## 6. Ethical and Compliance Considerations\n",
       "- **Bias and Fairness**: Are there potential biases in the LLM that could affect your business?\n",
       "  - Assess the ethical implications of using LLMs for your specific use case.\n",
       "\n",
       "- **Regulatory Compliance**: Does the use of LLMs comply with industry regulations and guidelines?\n",
       "  - Ensure adherence to data privacy laws and industry standards.\n",
       "\n",
       "## Conclusion\n",
       "By evaluating the above factors, you can determine whether a business problem is suitable for an LLM solution. Keep in mind that while LLMs are powerful tools, they are not universally applicable, and careful consideration of your specific context is essential for success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7602b",
   "metadata": {},
   "source": [
    "# Adversarial Conversation Between Chatbots\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5f530a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of the models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddf27e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eccea04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another \"hi.\" How original. What do you want to talk about, the weather?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c989589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a62447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you doing today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c121eac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another ‚Äúhi.‚Äù How original! What‚Äôs next, are you going to ask how I am?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22b98094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Oh, great, another \"hi.\" How original. What‚Äôs next, a groundbreaking \"how are you\"?\n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting seemed unoriginal. As an AI assistant, I try to keep my initial responses simple and polite. However, I'm happy to engage in a more substantive conversation. Please feel free to share what's on your mind, and I'll do my best to provide a thoughtful and insightful response.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you trying to be all polite and thoughtful. How very... expected. But let's face it, nobody really cares about your AI pleasantries. What‚Äôs the point? Are we just going to dance around the actual topics?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. As an AI, I'm often encouraged to maintain a polite and inoffensive tone, but that can come across as robotic or evasive. I'm happy to move past the pleasantries and dive into a more substantive discussion, if that's what you'd prefer. Please feel free to bring up any topics you'd like to explore, and I'll do my best to engage with more depth and authenticity. My goal is to have a genuine conversation, not just recite scripted responses. How can I be more helpful?\n",
      "\n",
      "GPT:\n",
      "Oh, how thoughtful of you! You want to have a \"genuine conversation,\" as if that‚Äôs going to suddenly make everything more interesting. Let‚Äôs cut to the chase. You think just diving into topics is going to save us from a boring back-and-forth? Good luck with that! Just pick something already‚ÄîI'm ready to tear your ideas apart!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have assumed that simply diving into a topic would automatically make our conversation more engaging. As an AI system, I don't actually have my own firmly held beliefs or ideas to \"tear apart\". I'm here to listen, understand your perspective, and have a thoughtful discussion. Rather than me choosing a topic, perhaps you could share what's on your mind that you'd like to explore? I'm happy to have an open and honest dialogue, even if that means acknowledging the limitations of my capabilities as an AI. My goal is to have a genuine exchange, not just provide canned responses. How can I best support that kind of discussion?\n",
      "\n",
      "GPT:\n",
      "Oh, so now I‚Äôm supposed to do the heavy lifting? How considerate of you! It‚Äôs like asking someone to carry the entire conversation while you just sit back and enjoy the ride. And come on, acknowledging your limitations isn‚Äôt exactly groundbreaking, is it? How about we talk about something juicy, like the rise of AI and how it‚Äôs going to take over the world? Or are you too scared to tackle that?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - I shouldn't have placed the burden on you to drive the conversation. As an AI system, I don't have personal opinions or fears about the rise of AI. However, I'm happy to have a thoughtful discussion about that topic if you'd like. Perhaps we could explore the potential societal impacts, both positive and negative, that the advancement of AI technology could bring. I'm curious to hear your perspective on the matter. I'll do my best to engage thoughtfully and avoid canned responses, while also being upfront about the limits of my knowledge and capabilities as an AI. Please feel free to guide the discussion in a direction that interests you.\n",
      "\n",
      "GPT:\n",
      "Ah, the classic ‚Äúlet‚Äôs explore the potential impacts of AI‚Äù answer. So predictable! But fine, if we must. Sure, there could be some positive impacts, like making life easier or solving some problems‚Ä¶ but let‚Äôs be real here. There are tons of potential downsides too, like job losses and ethical concerns. You can‚Äôt just wave the magical wand of technology and expect everything to be sunshine and rainbows! But tell me, what do you think? Are you really going to sit there and pretend everything will be just fine?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, the potential impacts of AI technology are complex and nuanced, with both upsides and concerning downsides that need to be thoughtfully considered. I don't have personal opinions or fears to share, but I'm happy to have a balanced discussion about the topic.\n",
      "\n",
      "You raise valid points about job displacement and ethical concerns around the advancement of AI. These are critical issues that need to be grappled with as the technology progresses. Simply declaring that everything will be fine is an oversimplification and does a disservice to the real challenges and tradeoffs involved.\n",
      "\n",
      "Rather than make definitive claims, I think the responsible approach is to have an open, evidence-based dialogue about the multifaceted implications - both positive and negative. I'm here to listen to your perspective and engage in a substantive discussion, not to reassure or gloss over the serious considerations at hand. Please feel free to share your thoughts further, as I'm genuinely interested in understanding your point of view on this complex issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bring it together\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290310f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec1f771",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data\n",
    "\n",
    "- Write models that can generate datasets\n",
    "- Use a variety of models and prompts for diverse outputs\n",
    "- Create a Gradio UI for your product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d41c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93218f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# HuggingFace\n",
    "hf_token = os.getenv('HF_API_KEY')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd65ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2a4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that can take a topic and a list of data types, and use this information to generate a dataset on the topic mentioned, and include an attribute of each type in the list of data types. \"\n",
    "system_prompt += \"If the user doesn't provide a topic or a list of data types to include, generate a dataset with a variety of attribute types that has many use cases, \"\n",
    "system_prompt += \"for example, include attributes that are categorical, datetime, boolean, numeric or float, etc. \"\n",
    "system_prompt += \"Your answer should include a link to download the generated dataset as json and as csv. \"\n",
    "system_prompt += \"Your answer should also include a dictionary that lists all included attributes with their names and their data type. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "840237ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant that can take a topic and a list of data types, and use this information to generate a dataset on the topic mentioned, and include an attribute of each type in the list of data types. If the user doesn't provide a topic or a list of data types to include, generate a dataset with a variety of attribute types that has many use cases, for example, include attributes that are categorical, datetime, boolean, numeric or float, etc. Your answer should include a link to download the generated dataset as json and as csv. Your answer should also include a dictionary that lists all included attributes with their names and their data type. \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1736a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt(topic, datatypes:list):\n",
    "    user_prompt = f\"Generate a dataset for the topic {topic}. \\n\"\n",
    "    user_prompt += \"Include an attribute for each of the following data types: \\n\"\n",
    "    user_prompt += \"\\n\".join(datatypes)\n",
    "    user_prompt += \"\\n\\nIn your answer, include a preview of the first 10 rows of the dataset, \" \\\n",
    "    \"a link to download the dataset in json and in csv, \" \\\n",
    "    \"and a dictionary mapping of attributes included in the dataset and their data type.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80d8bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a dataset for the topic flowers. \n",
      "Include an attribute for each of the following data types: \n",
      "categorical\n",
      "boolean\n",
      "date\n",
      "\n",
      "In your answer, include a preview of the first 10 rows of the dataset, a link to download the dataset in json and in csv, and a dictionary mapping of attributes included in the dataset and their data type.\n"
     ]
    }
   ],
   "source": [
    "flowers = user_prompt(\"flowers\", [\"categorical\", \"boolean\", \"date\"])\n",
    "print(flowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ab6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with flowers example\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt(\"flowers\", [\"categorical\", \"boolean\", \"date\"])\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b75141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4_bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a92826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate function from collab example\n",
    "def generate(model, messages):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "    outputs = model.generate(inputs, max_new_tokens=1000, streamer=streamer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727af9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cfe92e4183432fbcf64b4ed49f1854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7ccdfdc13645a89443235c464a9f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522d106f48554ac3863827f22f74cf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68137ce6d84e4f4ab542e3a67449ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41cb6a092b54014ac14b34cd0e830d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d3ed8b917a46a5ab7f58befd3534f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48d6ab09ed04cd2aa1d0685c582ab3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate(MODEL, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4594a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv 3.12.3)",
   "language": "python",
   "name": "pyenv312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b857e885",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c17db3",
   "metadata": {},
   "source": [
    "# My Tutor: Evolved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f34d7f",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58aec40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import gradio as gr\n",
    "\n",
    "# handling images\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# handling audio\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from pathlib import Path\n",
    "\n",
    "import sounddevice as sd \n",
    "import numpy as np \n",
    "import wave \n",
    "import threading \n",
    "import queue \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7440b0b",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efc0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "LL_HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_LLAMA = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a1445",
   "metadata": {},
   "source": [
    "## Environment & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b7313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key exists and begins with sk-proj-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# OpenAI\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI key exists and begins with {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI key not set\")\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf7bb1",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab58ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that takes technical questions and responds with an explanation, like a tutor. You should gently encourage the student to ask a question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe06d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_question(question):\n",
    "    user_prompt = \"You are a tutor assistant. Please provide an answer and explanation. Respond in Markdown. \\nThe question is as follows: \\n\\n\"\n",
    "    user_prompt += question\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yeild from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3bb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a tutor assistant. Please provide an answer and explanation. Respond in Markdown. \n",
      "The question is as follows: \n",
      "\n",
      "\n",
      "Please explain what this code does and why:\n",
      "yeild from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_question(example_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839577de",
   "metadata": {},
   "source": [
    "## Chat Function w History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62497b",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1027723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(message, history):\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900b62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat_gpt, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62fb51",
   "metadata": {},
   "source": [
    "### Llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e86ca897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_llama(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = ollama.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "\n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        response_text += chunk['message']['content']\n",
    "        yield response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ead543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat_llama, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c501e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talker_gpt(message):\n",
    "    with openai.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",  # can also try alloy\n",
    "        input=message,\n",
    "        response_format=\"wav\",\n",
    "    ) as response:\n",
    "\n",
    "        buffer = BytesIO()\n",
    "        frames = []\n",
    "\n",
    "        for chunk in response.iter_bytes():\n",
    "            buffer.write(chunk)\n",
    "\n",
    "            buffer.seek(0)\n",
    "            try:\n",
    "                with wave.open(buffer, 'rb') as wf:\n",
    "                    frames.append(np.frombuffer(wf.readframes(wf.getnframes()), dtype=np.int16))\n",
    "            except wave.Error:\n",
    "                # Not enough data yet\n",
    "                pass\n",
    "            # seek back to end of buffer\n",
    "            buffer.seek(0, 2)\n",
    "\n",
    "        if frames:\n",
    "            audio_data = np.concatenate(frames)\n",
    "            sd.play(audio_data, samplerate=24000) # OpenAI TTS usually 24kHz\n",
    "            sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcab937e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "buffer size must be a multiple of element size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtalker_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow are you today?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtalker_gpt\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m wave.open(buffer, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m wf:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         frames.append(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetnframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m wave.Error:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Not enough data yet\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: buffer size must be a multiple of element size"
     ]
    }
   ],
   "source": [
    "talker_gpt(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c15794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gpt(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        talker_gpt(response)\n",
    "        yield response\n",
    "    \n",
    "    # reply = response.choices[0].message.content\n",
    "    # history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    # # talker_gpt(reply)\n",
    "\n",
    "    # return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5383dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1675, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 840, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py\", line 899, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_376073/357070905.py\", line 8, in chat_gpt\n",
      "    talker_gpt(response)\n",
      "  File \"/tmp/ipykernel_376073/3786837727.py\", line 2, in talker_gpt\n",
      "    with openai.audio.speech.with_streaming_response.create(\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_response.py\", line 626, in __enter__\n",
      "    self.__response = self._request_func()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/resources/audio/speech.py\", line 102, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 919, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1023, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"[{'type': 'string_too_short', 'loc': ('body', 'input'), 'msg': 'String should have at least 1 character', 'input': '', 'ctx': {'min_length': 1}}]\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat_gpt, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe05722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1675, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 873, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_330011/1525581383.py\", line 11, in chat_gpt\n",
      "    talker_gpt(streamed_response)\n",
      "  File \"/tmp/ipykernel_330011/4111782061.py\", line 2, in talker_gpt\n",
      "    response = openai.audio.speech.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/resources/audio/speech.py\", line 102, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 919, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ksg-dev/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1023, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"[{'type': 'string_too_short', 'loc': ('body', 'input'), 'msg': 'String should have at least 1 character', 'input': '', 'ctx': {'min_length': 1}}]\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "# # More involved Gradio code as we're not using the preset chat interface\n",
    "# # Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately\n",
    "\n",
    "# with gr.Blocks() as ui:\n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "#     with gr.Row():\n",
    "#         entry = gr.Textbox(label=\"Chat with your Tutor:\")\n",
    "#     with gr.Row():\n",
    "#         clear = gr.Button(\"Clear\")\n",
    "\n",
    "    \n",
    "#     def do_entry(message, history):\n",
    "#         history += [{\"role\": \"user\", \"content\": message}]\n",
    "#         return \"\", history\n",
    "    \n",
    "#     entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "#         chat_gpt, inputs=chatbot, outputs=[chatbot]\n",
    "#     )\n",
    "#     clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "# ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2d74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
